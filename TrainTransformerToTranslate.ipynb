{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee4d965",
   "metadata": {},
   "source": [
    "# 1\tSubword tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d2dc3",
   "metadata": {},
   "source": [
    "## 1.1\tTokenize English and French phrases\n",
    "First go to https://gattonweb.uky.edu/faculty/lium/gai/en2fr.zip to download zip file that contains the 47,000 English to French translations that I collected from various sources. Unzip the file and place en2fr.csv in the folder /files/ on your computer. We'll load the data and take a look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aafd4b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 47173 examples in the training data\n",
      "How are you?\n",
      "Comment êtes-vous?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"files/en2fr.csv\")\n",
    "num_examples=len(df)\n",
    "print(f\"there are {num_examples} examples in the training data\")\n",
    "print(df.iloc[30856][\"en\"])\n",
    "print(df.iloc[30856][\"fr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a33bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Youse\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i</w>', 'don</w>', \"'t</w>\", 'speak</w>', 'fr', 'ench</w>', '.</w>']\n",
      "['je</w>', 'ne</w>', 'parle</w>', 'pas</w>', 'franc', 'ais</w>', '.</w>']\n",
      "['how</w>', 'are</w>', 'you</w>', '?</w>']\n",
      "['comment</w>', 'et', 'es-vous</w>', '?</w>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMTokenizer\n",
    "\n",
    "tokenizer = XLMTokenizer.from_pretrained(\"xlm-clm-enfr-1024\") # pretrained xlm model from hugging face\n",
    "\n",
    "tokenized_en=tokenizer.tokenize(\"I don't speak French.\")\n",
    "print(tokenized_en)\n",
    "tokenized_fr=tokenizer.tokenize(\"Je ne parle pas français.\")\n",
    "print(tokenized_fr)\n",
    "print(tokenizer.tokenize(\"How are you?\"))\n",
    "print(tokenizer.tokenize(\"Comment êtes-vous?\"))\n",
    "# the XLM model uses '</w>' as a token separator, except in cases where two tokens are part of the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7affb028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11053\n"
     ]
    }
   ],
   "source": [
    "# build dictionaries\n",
    "from collections import Counter\n",
    "\n",
    "en=df[\"en\"].tolist()\n",
    "\n",
    "en_tokens=[[\"BOS\"]+tokenizer.tokenize(x)+[\"EOS\"] for x in en]        \n",
    "PAD=0\n",
    "UNK=1\n",
    "# apply to English \n",
    "word_count=Counter()\n",
    "for sentence in en_tokens:\n",
    "    for word in sentence:\n",
    "        word_count[word]+=1\n",
    "frequency=word_count.most_common(50000)  # Keeps the 50,000 most frequent tokens only: list of tuples [(\"the\", 1200), (\"to\", 1100), (\"and\", 1050), ...]\n",
    "total_en_words=len(frequency)+2\n",
    "en_word_dict={w[0]:idx+2 for idx,w in enumerate(frequency)} # transform tokens to indices.\n",
    "en_word_dict[\"PAD\"]=PAD # the \"PAD\" token, used for padding, is allocated the integer 0,\n",
    "en_word_dict[\"UNK\"]=UNK #  the \"UNK\" token, representing unknown tokens, is given the integer 1.\n",
    "en_idx_dict={v:k for k,v in en_word_dict.items()} # transform inices back to english tokens\n",
    "\n",
    "print(len(frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c47c3d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BOS', 'two</w>', 'young</w>', ',</w>', 'white</w>', 'males</w>', 'are</w>', 'outside</w>', 'near</w>', 'many</w>', 'bus', 'hes</w>', '.</w>', 'EOS']\n",
      "[('a</w>', 52124), ('BOS', 47173), ('EOS', 47173), ('.</w>', 43237), ('in</w>', 16358)]\n",
      "3030\n",
      "1\n",
      "of</w>\n",
      "UNK\n",
      "['a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "UNK=1\n",
    "print(en_tokens[0])\n",
    "print(frequency[0:5])\n",
    "print(en_word_dict.get(\"a\", UNK))\n",
    "print(en_word_dict.get(\"axcd\", UNK))\n",
    "print(en_idx_dict.get(10,\"UNK\"))\n",
    "print(en_idx_dict.get(-1,\"UNK\"))\n",
    "print(['a']+['b']+['c'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d0abaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 100, 38, 377, 476, 574, 5]\n"
     ]
    }
   ],
   "source": [
    "enidx=[en_word_dict.get(i,UNK) for i in tokenized_en]  #If the token is not found, it assigns the index for \"UNK\" (1)\n",
    "print(enidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02572e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i</w>', 'don</w>', \"'t</w>\", 'speak</w>', 'fr', 'ench</w>', '.</w>']\n",
      "i don't speak french. \n"
     ]
    }
   ],
   "source": [
    "entokens=[en_idx_dict.get(i,\"UNK\") for i in enidx]   \n",
    "print(entokens)\n",
    "en_phrase=\"\".join(entokens)\n",
    "en_phrase=en_phrase.replace(\"</w>\",\" \") # Replaces the separator with a space\n",
    "for x in '''?:;.,'(\"-!&)%''': \n",
    "    en_phrase=en_phrase.replace(f\" {x}\",f\"{x}\")  # Removes the space before punctuations\n",
    "print(en_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f2bd162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for French phrases\n",
    "fr=df[\"fr\"].tolist()       \n",
    "fr_tokens=[[\"BOS\"]+tokenizer.tokenize(x)+[\"EOS\"] for x in fr] \n",
    "word_count=Counter()\n",
    "for sentence in fr_tokens:\n",
    "    for word in sentence:\n",
    "        word_count[word]+=1\n",
    "frequency=word_count.most_common(50000)        \n",
    "total_fr_words=len(frequency)+2\n",
    "fr_word_dict={w[0]:idx+2 for idx,w in enumerate(frequency)}\n",
    "fr_word_dict[\"PAD\"]=PAD\n",
    "fr_word_dict[\"UNK\"]=UNK\n",
    "fr_idx_dict={v:k for k,v in fr_word_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4e843fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 40, 231, 32, 726, 370, 4]\n"
     ]
    }
   ],
   "source": [
    "fridx=[fr_word_dict.get(i,UNK) for i in tokenized_fr]   \n",
    "print(fridx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8a78dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je</w>', 'ne</w>', 'parle</w>', 'pas</w>', 'franc', 'ais</w>', '.</w>']\n",
      "je ne parle pas francais. \n"
     ]
    }
   ],
   "source": [
    "frtokens=[fr_idx_dict.get(i,\"UNK\") for i in fridx]   \n",
    "print(frtokens)\n",
    "fr_phrase=\"\".join(frtokens)\n",
    "fr_phrase=fr_phrase.replace(\"</w>\",\" \")\n",
    "for x in '''?:;.,'(\"-!&)%''':\n",
    "    fr_phrase=fr_phrase.replace(f\" {x}\",f\"{x}\")  \n",
    "print(fr_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f77812fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"files/dict.p\",\"wb\") as fb:\n",
    "    pickle.dump((en_word_dict,en_idx_dict,fr_word_dict,fr_idx_dict),fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f214dff",
   "metadata": {},
   "source": [
    "## 1.2. Sequence Padding and Batch Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20480c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_en_ids=[[en_word_dict.get(w,UNK) for w in s] for s in en_tokens] # en_tokens = [[\"I\", \"love\", \"cats\"], [\"Hello\"]] => out_en_ids [[12, 45, 87], [34]]\n",
    "out_fr_ids=[[fr_word_dict.get(w,UNK) for w in s] for s in fr_tokens]\n",
    "sorted_ids=sorted(range(len(out_en_ids)), key=lambda x:len(out_en_ids[x])) \n",
    "# range(len(out_en_ids)) → creates indices for all sentences (0, 1, 2, ...).\n",
    "# key=lambda x: len(out_en_ids[x]) → sorts those indices by the length of the corresponding English sentence.\n",
    "# out_en_ids = [[12, 45, 87], [34]] -> sorted_ids = [1, 0]\n",
    "out_en_ids=[out_en_ids[x] for x in sorted_ids]\n",
    "out_fr_ids=[out_fr_ids[x] for x in sorted_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f328fd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47173\n",
      "['BOS', 'two</w>', 'young</w>', ',</w>', 'white</w>', 'males</w>', 'are</w>', 'outside</w>', 'near</w>', 'many</w>', 'bus', 'hes</w>', '.</w>', 'EOS']\n",
      "47173\n",
      "[3, 5168, 361, 4]\n",
      "[3, 2, 262, 185, 514, 45, 2061, 1055, 12, 896, 16, 7, 1102, 1094, 311, 12, 2, 31, 30, 12, 1504, 1847, 14, 2, 742, 3430, 867, 16, 7, 96, 11, 1094, 2, 680, 3761, 3762, 12, 293, 30, 16, 7, 96, 306, 17, 107, 5440, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "print(len(en_tokens))\n",
    "print(en_tokens[0])\n",
    "print(len(out_en_ids))\n",
    "print(out_en_ids[0])\n",
    "print(out_en_ids[47172])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfd4d7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of sentences pairs: 47173\n",
      "total number of sentenced pairs (ordered) 47173\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of sentences pairs:\",len(en_tokens))\n",
    "print(\"total number of sentenced pairs (ordered)\",len(out_en_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91845c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 369\n",
      "[39552 39553 39554 39555 39556 39557 39558 39559 39560 39561 39562 39563\n",
      " 39564 39565 39566 39567 39568 39569 39570 39571 39572 39573 39574 39575\n",
      " 39576 39577 39578 39579 39580 39581 39582 39583 39584 39585 39586 39587\n",
      " 39588 39589 39590 39591 39592 39593 39594 39595 39596 39597 39598 39599\n",
      " 39600 39601 39602 39603 39604 39605 39606 39607 39608 39609 39610 39611\n",
      " 39612 39613 39614 39615 39616 39617 39618 39619 39620 39621 39622 39623\n",
      " 39624 39625 39626 39627 39628 39629 39630 39631 39632 39633 39634 39635\n",
      " 39636 39637 39638 39639 39640 39641 39642 39643 39644 39645 39646 39647\n",
      " 39648 39649 39650 39651 39652 39653 39654 39655 39656 39657 39658 39659\n",
      " 39660 39661 39662 39663 39664 39665 39666 39667 39668 39669 39670 39671\n",
      " 39672 39673 39674 39675 39676 39677 39678 39679]\n",
      "[30208 30209 30210 30211 30212 30213 30214 30215 30216 30217 30218 30219\n",
      " 30220 30221 30222 30223 30224 30225 30226 30227 30228 30229 30230 30231\n",
      " 30232 30233 30234 30235 30236 30237 30238 30239 30240 30241 30242 30243\n",
      " 30244 30245 30246 30247 30248 30249 30250 30251 30252 30253 30254 30255\n",
      " 30256 30257 30258 30259 30260 30261 30262 30263 30264 30265 30266 30267\n",
      " 30268 30269 30270 30271 30272 30273 30274 30275 30276 30277 30278 30279\n",
      " 30280 30281 30282 30283 30284 30285 30286 30287 30288 30289 30290 30291\n",
      " 30292 30293 30294 30295 30296 30297 30298 30299 30300 30301 30302 30303\n",
      " 30304 30305 30306 30307 30308 30309 30310 30311 30312 30313 30314 30315\n",
      " 30316 30317 30318 30319 30320 30321 30322 30323 30324 30325 30326 30327\n",
      " 30328 30329 30330 30331 30332 30333 30334 30335]\n",
      "47104\n",
      "47173\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "batch_size=128 # batch size is 128 tokens\n",
    "idx_list=np.arange(0,len(en_tokens),batch_size) # include the indices which are the starts of all bacthes\n",
    "np.random.shuffle(idx_list)\n",
    "\n",
    "batch_indexs=[] # Each elemnnt in this list is a list that includes 128 of adjacant indices of sentences pairs in the csv file\n",
    "for idx in idx_list: # Iterates over each batch starting index (idx)\n",
    "    batch_indexs.append(np.arange(idx, min(len(en_tokens), idx+batch_size)))\n",
    "\n",
    "print(f\"number of batches: {len(batch_indexs)}\")\n",
    "print(batch_indexs[0]) # first batch of sentences\n",
    "print(batch_indexs[1]) # second batch\n",
    "print(np.max(idx_list))\n",
    "print(min(len(en_tokens), 47104+batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bec238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_padding(X, padding=0):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = np.max(L) # Find out the length of the longest sequence in the batch\n",
    "    padded_seq = np.array([np.concatenate([x, [padding] * (ML - len(x))])\n",
    "                            if len(x) < ML else x for x in X]) #If a batch is shorter than the longest sequence, add 0s to the sequence at the end.\n",
    "    return padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db2e1fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 7, 23, 24, 5, 213, 9, 188, 254, 94, 5, 262, 1051, 8, 149, 970, 4, 3], [2, 5, 17, 24, 5, 395, 251, 1063, 19, 1356, 14, 7, 60, 526, 241, 487, 8, 13, 157, 4, 3], [2, 1415, 7, 598, 8, 25, 43, 56, 21, 2758, 12, 2977, 10, 6, 5, 6299, 8, 916, 4, 3]]\n",
      "max: 31, min: 14\n",
      "[[   2    7   23   24    5  213    9  188  254   94    5  262 1051    8\n",
      "   149  970    4    3    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   2    5   17   24    5  395  251 1063   19 1356   14    7   60  526\n",
      "   241  487    8   13  157    4    3    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   2 1415    7  598    8   25   43   56   21 2758   12 2977   10    6\n",
      "     5 6299    8  916    4    3    0    0    0    0    0    0    0    0\n",
      "     0    0    0]]\n",
      "(128, 31)\n"
     ]
    }
   ],
   "source": [
    "sentneces_indices_in_one_batch= batch_indexs[0]\n",
    "batch_tokens_indices = [out_fr_ids[sentenc_index] for sentenc_index in sentneces_indices_in_one_batch]\n",
    "print(batch_tokens_indices[0:3])\n",
    "\n",
    "L = [len(x) for x in batch_tokens_indices]\n",
    "print(f\"max: {np.max(L)}, min: {np.min(L)}\")\n",
    "\n",
    "padded_batch = seq_padding(batch_tokens_indices)\n",
    "print(padded_batch[0:3])\n",
    "print(padded_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e37251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import Batch\n",
    "\n",
    "# The BatchLoader() class creates data batches intended for training. Each batch in \n",
    "# this list contains 128 pairs, where each pair contains numerical representations of an \n",
    "# English phrase and its corresponding French translation\n",
    "\n",
    "# batch_indexes: Each elemnnt in this list is a list that includes 128 of adjacant indices of sentences pairs in the en2fr.csv file.\n",
    "# out_en_ids: takes a sentence index and map it to the correspponding sentnce numerical representation, since sentnces indices are adjacant,\n",
    "# the entire batch sentnces lengthes would be very close to each other, because out_en_ids orders sentnce ascendingly accoridng to their lengthes.\n",
    "class BatchLoader():\n",
    "    def __init__(self):\n",
    "        self.idx=0 # self.idx is used to keep track of which batch number we’re currently on.\n",
    "    def __iter__(self): # This makes the object iterable.\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        self.idx += 1\n",
    "        if self.idx<=len(batch_indexs):\n",
    "            b=batch_indexs[self.idx-1]\n",
    "            batch_en=[out_en_ids[x] for x in b] # [[12, 45, 87], [34, 66],[7, 18, 9],....] 128 english sentences (each is a list of token indices) \n",
    "            batch_fr=[out_fr_ids[x] for x in b]  # [[17, 4], [1, 7],[3, 9],....] 128 french sentences (each is a list of token indices)\n",
    "            batch_en=seq_padding(batch_en) # append 0's to each sentneces (128, sql_len)\n",
    "            batch_fr=seq_padding(batch_fr) # append 0's to each sentneces (128, sql_len)\n",
    "            return Batch(batch_en,batch_fr)\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60bba954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   3,    2,   19,   14,    2,  841,  339,  175,  314,    2,  744,  737,\n",
      "         257,   10,   82, 1047,    5,    4])\n",
      "['BOS', 'a</w>', 'woman</w>', 'with</w>', 'a</w>', 'hand', 'bag</w>', 'walks</w>', 'past</w>', 'a</w>', 'chin', 'ese</w>', 'store</w>', 'of</w>', 'some</w>', 'sort</w>', '.</w>', 'EOS']\n",
      "BOSa woman with a handbag walks past a chinese store of some sort. EOS\n",
      "tensor([   2,    7,   23,   24,    5,  213,    9,  188,  254,   94,    5,  262,\n",
      "        1051,    8,  149,  970,    4,    3,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "['BOS', 'une</w>', 'femme</w>', 'avec</w>', 'un</w>', 'sac</w>', 'a</w>', 'main</w>', 'passe</w>', 'par</w>', 'un</w>', 'magasin</w>', 'chinois</w>', 'de</w>', 'quelque</w>', 'sorte</w>', '.</w>', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "BOSune femme avec un sac a main passe par un magasin chinois de quelque sorte. EOSPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Source sequence shape (batch_size, seq_len): torch.Size([128, 18])\n",
      "First source sequence tokens: tensor([   3,    2,   19,   14,    2,  841,  339,  175,  314,    2,  744,  737,\n",
      "         257,   10,   82, 1047,    5,    4])\n",
      "================> Source mask shape (batch_size, 1, seq_len): torch.Size([128, 1, 18])\n",
      "First target input sequence (decoder input): tensor([   2,    7,   23,   24,    5,  213,    9,  188,  254,   94,    5,  262,\n",
      "        1051,    8,  149,  970,    4,    3,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "Target input shape (batch_size, seq_len): torch.Size([128, 30])\n",
      "First target output sequence (ground truth): tensor([   7,   23,   24,    5,  213,    9,  188,  254,   94,    5,  262, 1051,\n",
      "           8,  149,  970,    4,    3,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "Target output shape (batch_size, seq_len): torch.Size([128, 30])\n",
      "================> Target mask shape (batch_size, seq_len, seq_len): torch.Size([128, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "loader = BatchLoader()\n",
    "first_batch = next(loader)   \n",
    "\n",
    "en_sentence_numbered = first_batch.src[0]\n",
    "print(en_sentence_numbered)\n",
    "entokens=[en_idx_dict.get(i.item(),\"UNK\") for i in en_sentence_numbered]   \n",
    "print(entokens)\n",
    "en_phrase=\"\".join(entokens)\n",
    "en_phrase=en_phrase.replace(\"</w>\",\" \") \n",
    "for x in '''?:;.,'(\"-!&)%''': \n",
    "    en_phrase=en_phrase.replace(f\" {x}\",f\"{x}\")  \n",
    "print(en_phrase)\n",
    "\n",
    "\n",
    "fr_sentence_numbered = first_batch.trg[0]\n",
    "print(fr_sentence_numbered)\n",
    "frtokens=[fr_idx_dict.get(i.item(),\"UNK\") for i in fr_sentence_numbered]   \n",
    "print(frtokens)\n",
    "fr_phrase=\"\".join(frtokens)\n",
    "fr_phrase=fr_phrase.replace(\"</w>\",\" \")\n",
    "for x in '''?:;.,'(\"-!&)%''':\n",
    "    fr_phrase=fr_phrase.replace(f\" {x}\",f\"{x}\")  \n",
    "print(fr_phrase)\n",
    "\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(\"Source sequence shape (batch_size, seq_len):\", first_batch.src.shape)\n",
    "print(\"First source sequence tokens:\", first_batch.src[0])\n",
    "print(\"================> Source mask shape (batch_size, 1, seq_len):\", first_batch.src_mask.shape)\n",
    "print(\"First target input sequence (decoder input):\", first_batch.trg[0])\n",
    "print(\"Target input shape (batch_size, seq_len):\", first_batch.trg.shape)  # last token dropped\n",
    "print(\"First target output sequence (ground truth):\", first_batch.trg_y[0])\n",
    "print(\"Target output shape (batch_size, seq_len):\", first_batch.trg_y.shape)  # first token dropped\n",
    "print(\"================> Target mask shape (batch_size, seq_len, seq_len):\", first_batch.trg_mask.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450f4fe",
   "metadata": {},
   "source": [
    "# 2\tWord embedding and positional encoding\n",
    "## 2.1. Word Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05d352a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 11055 distinct English tokens\n",
      "there are 11239 distinct French tokens\n"
     ]
    }
   ],
   "source": [
    "src_vocab = len(en_word_dict)\n",
    "tgt_vocab = len(fr_word_dict)\n",
    "print(f\"there are {src_vocab} distinct English tokens\")\n",
    "print(f\"there are {tgt_vocab} distinct French tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f17ecb",
   "metadata": {},
   "source": [
    "## 2.1. Positional Encoding\n",
    "To model the order of elements in the input and output sequences, we'll first create positional encodings of the sequences as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dffa6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of positional encoding is torch.Size([128, 25, 256])\n"
     ]
    }
   ],
   "source": [
    "from util import PositionalEncoding\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "pe = PositionalEncoding(256, 0.1)\n",
    "x = torch.zeros(128, 25, 256).to(DEVICE) #  Creates a word embedding and fills it with zeros\n",
    "y = pe.forward(x) # Calculates the input embedding by adding positional encoding to the word embedding\n",
    "print(f\"the shape of positional encoding is {y.shape}\")\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df1b8ba",
   "metadata": {},
   "source": [
    "# 3\tTrain the Transformer for English-to-French translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4768df6",
   "metadata": {},
   "source": [
    "## 3.1 Loss Function and the Optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22e51429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import create_model\n",
    "\n",
    "model = create_model(src_vocab, tgt_vocab, N=6, d_model=256, d_ff=1024, h=8, dropout=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073dab3",
   "metadata": {},
   "source": [
    "We create the optimizer for training as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29a4ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import NoamOpt\n",
    "\n",
    "optimizer = NoamOpt(256, 1, 2000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5163f7f",
   "metadata": {},
   "source": [
    "We then define the loss function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b0f9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import (LabelSmoothing, SimpleLossCompute)\n",
    "\n",
    "criterion = LabelSmoothing(tgt_vocab, padding_idx=0, smoothing=0.1)\n",
    "loss_func = SimpleLossCompute(model.generator, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062599ec",
   "metadata": {},
   "source": [
    "## 3.2 The training loop\n",
    "We'll train the model for 100 epochs. We'll calculate the loss and the number of tokens from each batch. After each epoch, we calculate the average loss in the epoch as the ratio between the total loss and the total number of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0146ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for 100 epochs\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    tloss=0\n",
    "    tokens=0\n",
    "    for batch in BatchLoader():\n",
    "        out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_func(out, batch.trg_y, batch.ntokens)\n",
    "        tloss += loss\n",
    "        tokens += batch.ntokens\n",
    "    print(f\"Epoch {epoch}, average loss: {tloss/tokens}\")\n",
    "torch.save(model.state_dict(),\"files/en2fr.pth\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3274397",
   "metadata": {},
   "source": [
    "The above training process takes a couple of hours if you are using a GPU. It may take several hours if you are using CPU training. Once the training is done, the model weights are saved as *en2fr.pth* on your computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83ef71",
   "metadata": {},
   "source": [
    "# 4. Translate English to French with the Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4779970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import subsequent_mask\n",
    "\n",
    "\n",
    "def translate(eng):\n",
    "    # tokenize the English sentence\n",
    "    tokenized_en=tokenizer.tokenize(eng)\n",
    "    # add beginning and end tokens\n",
    "    tokenized_en=[\"BOS\"]+tokenized_en+[\"EOS\"]\n",
    "    # convert tokens to indexes\n",
    "    enidx=[en_word_dict.get(i,UNK) for i in tokenized_en]  \n",
    "    src=torch.tensor(enidx).long().to(DEVICE).unsqueeze(0)\n",
    "    # create mask to hide padding\n",
    "    src_mask=(src!=0).unsqueeze(-2)\n",
    "    # encode the English sentence\n",
    "    memory=model.encode(src,src_mask)\n",
    "    # start translation in an autogressive fashion\n",
    "    start_symbol=fr_word_dict[\"BOS\"]\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    translation=[]\n",
    "    for i in range(100):\n",
    "        out = model.decode(memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)) # Generate next token\n",
    "        prob = model.generator(out[:, -1]) # # Get probability distribution for next word\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1) # Append to growing sequence\n",
    "        sym = fr_idx_dict[ys[0, -1].item()] # Convert index to actual word\n",
    "        if sym != 'EOS':\n",
    "            translation.append(sym)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    # convert tokens to sentences\n",
    "    trans=\"\".join(translation)\n",
    "    trans=trans.replace(\"</w>\",\" \") \n",
    "    for x in '''?:;.,'(\"-!&)%''':\n",
    "        trans=trans.replace(f\" {x}\",f\"{x}\")    \n",
    "    print(trans)\n",
    "    return trans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e8c99d",
   "metadata": {},
   "source": [
    "Let's try the defined function on the English phrase \"Today is a beautiful day!\", like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2af177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aujourd'hui est une belle journee! \n"
     ]
    }
   ],
   "source": [
    "with open(\"files/dict.p\",\"rb\") as fb:\n",
    "    en_word_dict,en_idx_dict,\\\n",
    "    fr_word_dict,fr_idx_dict=pickle.load(fb)\n",
    "trained_weights=torch.load(\"files/en2fr.pth\",map_location=DEVICE)\n",
    "model.load_state_dict(trained_weights)\n",
    "model.eval()\n",
    "eng = \"Today is a beautiful day!\"\n",
    "translated_fr = translate(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4fea34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un petit garcon en jeans grimpe un petit arbre tandis qu'un autre enfant regarde. \n"
     ]
    }
   ],
   "source": [
    "eng = \"A little boy in jeans climbs a small tree while another child looks on.\"\n",
    "translated_fr = translate(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d060f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je ne parle pas francais. \n"
     ]
    }
   ],
   "source": [
    "eng = \"I don't speak French.\"\n",
    "translated_fr = translate(eng) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f342772",
   "metadata": {},
   "source": [
    "Now let's try the sentence \"I do not speak French.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f8649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je ne parle pas francais. \n"
     ]
    }
   ],
   "source": [
    "eng = \"I do not speak French.\"\n",
    "translated_fr = translate(eng)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
